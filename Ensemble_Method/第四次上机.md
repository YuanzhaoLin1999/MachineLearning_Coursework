[*林远钊 1700010672*]{}

## 机器学习第4次上机作业

[TOC]

### 集成学习Review

#### 基本介绍

集成学习（ensemble learning）通过构建并结合读个学习器来完成学习的任务。集成学习主要有两类：

一类是通过平均方法（即Bagging算法），这种方法的思想是通过构建几个独立的估计学习期并且平均他们的预测来得到结果。平均而言，组合后的估计要好于其中任何一个基学习器的估计。

另一类是提升方法，按照一个序列构造基学习器并且尝试在这个过程中减少聚合学习结果的偏误，其背后的思想是通过集成几个弱学习器来形成一个有效的预测器。

值得一提的是，Bagging方法适用于较复杂的、强的模型，因为他能够有效降低过拟合；而Boosting方法则一般对弱学习器更有效。

#### 问题叙述

MultiBoosting 算法将 AdaBoost 作为 Bagging 的基学习器，Iterative Bagging算法则是将Bagging作为AdaBoost的基学习器。实现并比较二者的优缺点。

#### 实现工具

本次上机作业中主要通过scikit-learn这一python包来实现支持向量机。在使用之前需要对其进行一定的了解和学习。

sklearn(scikit-learn包的常用缩写)中，Bagging方法有 BaggingClassifier和BaggingRegressor两个类可以实现，它们接受一个由使用者设定的基学习器参数和一个代表随机选择训练集的方案的参数。以k近邻方法为基学习器的官方代码示例如下

```python
from sklearn.ensemble import BaggingClassifier
from sklearn.neighbors import KNeighborsClassifier
bagging = BaggingClassifier(KNeighborsClassifier(),
                max_samples=0.5, max_features=0.5)
```

sklear中的sklearn.ensemble 模块中包含了较流行的提升算法如AdaBoost。

在正式使用这两个包实现本次问题前，先利用官方给出的示例对AdaBoost和Bagging算法进行简单的应用，对一个生成的数据给出学习结果如下

![AdaBoost](L:\pku\专业课\机器学习基础 牟克典\Code\Ensemble Method\AdaBoost.png)

![Bagging](L:\pku\专业课\机器学习基础 牟克典\Code\Ensemble Method\Bagging.png)

---

### python实现

由于本次上机中使用了包，大可把其作为黑箱，其中具体实现细节不必深究。以下比较AdaBoost、Bagging、MultiAdaBoost、Iterative Bagging算法在分类和回归问题中的性能。

---

#### 分类问题

首先在官方网站的示例代码的基础上进行一个二分类任务的学习和可视化任务，从而对AdaBoost、Bagging、MultiAdaBoosting和Iterative Bagging四种算法有一些基本的了解

分别运行四种算法得到：

-   AdaBoost 算法耗时0.06894421577453613 s

-   Bagging 算法耗时0.01584601402282715 s

-   MultiAdaBoosting算法耗时0.6348793506622314 s

-   Iter Bagging算法耗时3.324193239212036 s

其得到的分类边界图分别为

综合以上信息能够得到一些简单的结果——四种算法中 AdaBoost算法得到的模型预测效果稍差（在(a)图中可以看出，在A、B两类混合的区域中分类效果较差），并且IterBagging 算法的运行时间显著长于其他三种。

---

##### Breast Cancer数据集

接下来，对于一个实际的数据集——Breast\_Cancer数据集，进行分类问题的训练，并且通过10折验证来得到习得模型的性能的度量。

首先给出算法的核心代码：

``` python
X = load_breast_cancer().data
y = load_breast_cancer().target
bdt = AdaBoostClassifier(BaggingClassifier(), n_estimators = 200)
#bdt = AdaBoostClassifier()
#bdt = BaggingClassifier()
#bdt = BaggingClassifier(AdaBoostClassifier(), 
#        max_samples = 0.63, max_features = 0.63)
bdt.fit(X, y)
```

分别运行四种算法得到运行结果

**AdaBoost**

![AdaBoost](C:/Users/linxi/AppData/Roaming/Typora/typora-user-images/1559064164043.png)

**Bagging**

![Bagging](C:/Users/linxi/AppData/Roaming/Typora/typora-user-images/1559064196197.png)

**MultiAdaBoost**

![MultiAdaBoosting](C:/Users/linxi/AppData/Roaming/Typora/typora-user-images/1559064287921.png)

**Iterative Bagging**

![Iter Bagging](C:/Users/linxi/AppData/Roaming/Typora/typora-user-images/1559064887776.png)



从这些数据可以得到一些基本的结果：

- MultiAdaBoost和Iterative Bagging算法的运行时间显著长于AdaBoost、Bagging算法（这是显然符合事实的
- Iterative Bagging 和 MultiAdaBoost算法相比，在10折验证的过程中，波动（方差）显著高于MultiAdaBoosting算法，最终的预测正确率则相差差不多，和两个基算法比起来，学习率略有上升

---

##### wine数据集

类似的，我们使用wine数据集，再对四种算法进行测试。

**Iterative Bagging**

![Iterative Bagging](C:/Users/linxi/AppData/Roaming/Typora/typora-user-images/1559115249973.png)

**MultiAdaBoosting**

![MultiBoosting](C:/Users/linxi/AppData/Roaming/Typora/typora-user-images/1559115357326.png)

**AdaBoosting**

![AdaBoosting](C:/Users/linxi/AppData/Roaming/Typora/typora-user-images/1559115398760.png)

**Bagging**

![Bagging](C:/Users/linxi/AppData/Roaming/Typora/typora-user-images/1559115425323.png)

- 在这个数据集中Bagging算法获得了出人意料的高学习率
- 在运行所需时间的方面，此时MultiAdaBoosting算法所需时间显著多于其他三个算法。

---



#### 回归问题

在以下回归问题中，评分（score）的标准暂定为 $R^2$ 

##### Boston数据集

我们首先使用Boston房价数据集对四种算法在回归问题中的表现进行一些测试。 依次展示 AdaBoost、Bagging、MultiAdaBoosting、Iterative Bagging的运行结果

**AdaBoost**

![AdaBoost](C:/Users/linxi/AppData/Roaming/Typora/typora-user-images/1559111549233.png)

**Bagging**

![Bagging](C:/Users/linxi/AppData/Roaming/Typora/typora-user-images/1559111623025.png)

**MultiAdaBoost**

![MultiAda](C:/Users/linxi/AppData/Roaming/Typora/typora-user-images/1559114602851.png)

**Iterative Bagging**

![Iterative Bagging](C:/Users/linxi/AppData/Roaming/Typora/typora-user-images/1559114001782.png)

同时，还比较了决策树桩、AdaBoost算法、MultiAdaBoosting和Iterative Bagging算法 的预测值和真实值之差![Boston_all](L:\pku\专业课\机器学习基础 牟克典\Code\Ensemble Method\Boston_all.png)

- 在回归问题的第一次尝试中，发现MultiAdaBoost算法所耗时间平均比其余三个高一个数量级
- 本次回归问题中，新的两个算法的学习效果似乎并不好

---

##### Diabetes数据集

再尝试一下Diabetes数据集的算法运行结果

展示的运行结果分别是MultiBoosting、Iterative Bagging、Bagging、AdaBoosting

**MultiAdaBoost**

![MultiBoosting](C:/Users/linxi/AppData/Roaming/Typora/typora-user-images/1559116491846.png)

**Iterative Bagging**

![Iterative Bagging](C:/Users/linxi/AppData/Roaming/Typora/typora-user-images/1559116565202.png)

**Bagging**

![Bagging](C:/Users/linxi/AppData/Roaming/Typora/typora-user-images/1559116603792.png)

**AdaBoost**

![AdaBoosting](C:/Users/linxi/AppData/Roaming/Typora/typora-user-images/1559116654485.png)

---

- 在此次回归问题中，新的 两个算法的解释力比起原本的有一定提升，但并不十分显著
- MultiAdaBoosting 算法的运行时间问题仍然突出

### 总结和思考

在所有实验开始之前，依照笔者原本的考虑：

MultiAdaBoost是以提升算法AdaBoost作为Bagging的基学习器，这个算法理当有更好的效果。因为一般情况下，Bagging方法适用于较复杂的、强的模型，因为他能够有效降低过拟合；而Boosting方法则一般对弱学习器更有效，因为它的长处在于将弱学习器提升增强。

因而，若以AdaBoost为基础，能为Bagging算法提供一个较强的已有学习器，再由Bagging算法降低过拟合风险，可以得到较好的预测效果；而反之，若以Bagging算法为基学习器，而Bagging算法又以默认的不太强的学习算法训练，这妨碍了Bagging算法发挥其本应有的功效。而通过Bagging算法得到的较强的基学习器，又不是提升算法所需要的，获得性能提升并不明显，因而MultiAdaBoost算法当显著好于Iterative Bagging算法。



但是在对几个问题进行实验之后，发现并没有证据可以支持笔者的这个想法，出现了一些数据上的其他的共同点

- Iterative Bagging算法学习率的方差普遍偏大
- MultiAdaBoosting 算法的运行时间普遍比其他算法长一个数量级
- Iterative Bagging 和MultiAdaBoosting算法和原本的AdaBoost算法和Bagging算法比起来，并没有显著的性能提升（这一点倒可以理解，本身AdaBoost和Bagging算法的学习效果并不差，进一步提升比较困难）



虽然最开始做了一些看起来似乎有道理的考虑，但是似乎并未能从实验数据中找到支持。最后基于几组实验结果，观察数据中表现出的一些共同点，给出了一些可能的方向。

同时，也发现了一些需要努力的方向——比如对实验数据的处理还比较迷茫，并不知道有哪些量是应当研究的，只选取了一些简单的数据。再比如对于自己总结出的共同点还不能给出一个合理的解释。



（PS：由于本次作业中图片较多，使用Latex 排版非常乱，因而转用markdown，比起Latex 可能不是那么正式，向助教学长表示抱歉qwq）