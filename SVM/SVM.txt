Support vector machines(SVMS) are a set of supervise learning 
methods used for classification, regression and outliers detection.

The advantages of support vector machines are:
1. Effective in high dimensional spaces
2. Still effective in cases where number of dimensions is greater 
than the number of samples.
3. Uses a subset of training points in the decision function (
    so called support vectors), so it's also memory effecient.
4. Versatile:（通用的，多面手的） different Kernel Functions can be 
specified for the decision function. Common kernels are provided, but
 it is also possible to specify custom kernels. 

 The disadvantages of SVM are:
 1. if the number of features is much greater than the number of samples,
 avoid over-fitting in choosing Kernel functions and regularization 
 term is crucial. 
 2. SVMs do not directly provide probability estimates, these 
 are calculated using an expensive five-fold cross-validation

 sklearn support both dense(稠密) and sparse（稀疏的 scipy.sparse）sample 
 vectors as input. 
 （稀疏矩阵：若数值为0的元素数目远多于非0的，且非0元素分布无规律）

Part One:
SVC, NuSVC and Linear SVC 都可以进行多分类。
SVC和NuSVC是相似的方法，但是接受的参数集稍有不同且有不同的数理公式形式。
另一方面，LinearSVC是另一种线性核函数支持向量分类的实现。注意LinearSVC不接受
keyword kernel，因为它总是被假设是线性的。

和其他分类器一样，以上三种方法接受两个array作为输出，an array X of size [n_
samples, n_features] holding the training samples, and an 
array y of class labels (strings or integers), size [n_sampl
es]
(SVC参数表 kernel:算法中将使用的核函数。gamma:核系数。)

多分类问题：
SVC和NuSVC采用了逐对分类的方法。如果有n个类，那么需要建立n(n-1)/2个分类器，
每个都训练两个类当中的数据。为了提供一个和其他分类器一致的接口，decision_function_shape
选项可以聚合逐对分类的结果到一个决策函数。
